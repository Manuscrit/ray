{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Model selection and serving with Ray Tune and Ray Serve\nThis tutorial will show you an end-to-end example how to train a\nmodel using Ray Tune on incrementally arriving data and deploy\nthe model using Ray Serve.\n\nA machine learning workflow can be quite simple: You decide on\nthe objective you're trying to solve, collect and annotate the\ndata, and build a model to hopefully solve your problem. But\nusually the work is not over yet. First, you would likely continue\nto do some hyperparameter optimization to obtain the best possible\nmodel (called *model selection*). Second, your trained model\nsomehow has to be moved to production - in other words, users\nor services should be enabled to use your model to actually make\npredictions. This part is called *model serving*.\n\nFortunately, Ray includes two libraries that help you with these\ntwo steps: Ray Tune and Ray Serve. And even more, they compliment\neach other nicely. Most notably, both are able to scale up your\nworkloads easily - so both your model training and serving benefit\nfrom additional resources and can adapt to your environment. If you\nneed to train on more data or have more hyperparameters to tune,\nRay Tune can leverage your whole cluster for training. If you have\nmany users doing inference on your served models, Ray Serve can\nautomatically distribute the inference backends to multiple nodes.\n\nThis tutorial will show you an end-to-end example how to train a MNIST\nimage classifier on incrementally arriving data and automatically\nserve an updated model on a HTTP endpoint.\n\nBy the end of this tutorial you will be able to\n\n1. Do hyperparameter optimization on a simple MNIST classifier\n2. Continue to train this classifier from an existing model with\n   newly arriving data\n3. Automatically create and serve data backends with Ray Serve\n\n## Roadmap and desired functionality\nThe general idea of this example is that we simulate newly arriving\ndata each day. So at day 0 we might have some initial data available\nalready, but at each day, new data arrives.\n\nOur approach here is that we offer two ways to train: From scratch and\nfrom an existing model. Maybe you would like to train and select models\nfrom scratch each week with all data available until then, e.g. each\nSunday, like this:\n\n.. code-block:: bash\n\n    # Train with all data available at day 0\n    python tune-serve-integration-mnist.py --from_scratch --day 0\n\nDuring the other days you might want to improve your model, but\nnot train everything from scratch, saving some cluster resources.\n\n.. code-block:: bash\n\n    # Train with data arriving between day 0 and day 1\n    python tune-serve-integration-mnist.py --from_existing --day 1\n    # Train with incremental data on the other days, too\n    python tune-serve-integration-mnist.py --from_existing --day 2\n    python tune-serve-integration-mnist.py --from_existing --day 3\n    python tune-serve-integration-mnist.py --from_existing --day 4\n    python tune-serve-integration-mnist.py --from_existing --day 5\n    python tune-serve-integration-mnist.py --from_existing --day 6\n    # Retrain from scratch every 7th day:\n    python tune-serve-integration-mnist.py --from_scratch --day 7\n\nThis example will support both modes. After each model selection run,\nwe will tell Ray Serve to serve an updated model. We also include a\nsmall utility to query our served model to see if it works as it should.\n\n.. code-block:: bash\n\n    $ python tune-serve-integration-mnist.py --query 6\n    Querying model with example #6. Label = 1, Response = 1, Correct = True\n\n## Imports\nLet's start with our dependencies. Most of these should be familiar\nif you worked with PyTorch before. The most notable import for Ray\nis the ``from ray import tune, serve`` import statement - which\nincludes almost all the things we need from the Ray side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\nimport json\nimport os\nimport shutil\nimport sys\nfrom functools import partial\nfrom math import ceil\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport ray\nfrom ray import tune, serve\nfrom ray.serve.exceptions import RayServeException\nfrom ray.tune import CLIReporter\nfrom ray.tune.schedulers import ASHAScheduler\n\nfrom torch.utils.data import random_split, Subset\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data interface\nLet's start with a simulated data interface. This class acts as the\ninterface between your training code and your database. We simulate\nthat new data arrives each day with a ``day`` parameter. So, calling\n``get_data(day=3)`` would return all data we received until day 3.\nWe also implement an incremental data method, so calling\n``get_incremental_data(day=3)`` would return all data collected\nbetween day 2 and day 3.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MNISTDataInterface(object):\n    \"\"\"Data interface. Simulates that new data arrives every day.\"\"\"\n\n    def __init__(self, data_dir, max_days=10):\n        self.data_dir = data_dir\n        self.max_days = max_days\n\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ])\n        self.dataset = MNIST(\n            self.data_dir, train=True, download=True, transform=transform)\n\n    def _get_day_slice(self, day=0):\n        if day < 0:\n            return 0\n        n = len(self.dataset)\n        # Start with 30% of the data, get more data each day\n        return min(n, ceil(n * (0.3 + 0.7 * day / self.max_days)))\n\n    def get_data(self, day=0):\n        \"\"\"Get complete normalized train and validation data to date.\"\"\"\n        end = self._get_day_slice(day)\n\n        available_data = Subset(self.dataset, list(range(end)))\n        train_n = int(0.8 * end)  # 80% train data, 20% validation data\n\n        return random_split(available_data, [train_n, end - train_n])\n\n    def get_incremental_data(self, day=0):\n        \"\"\"Get next normalized train and validation data day slice.\"\"\"\n        start = self._get_day_slice(day - 1)\n        end = self._get_day_slice(day)\n\n        available_data = Subset(self.dataset, list(range(start, end)))\n        train_n = int(\n            0.8 * (end - start))  # 80% train data, 20% validation data\n\n        return random_split(available_data, [train_n, end - start - train_n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTorch neural network classifier\nNext, we will introduce our PyTorch neural network model and the\ntrain and test function. These are adapted directly from\nour :doc:`PyTorch MNIST example </tune/examples/mnist_pytorch>`.\nWe only introduced an additional neural network layer with a configurable\nlayer size. This is not strictly needed for learning good performance on\nMNIST, but it is useful to demonstrate scenarios where your hyperparameter\nsearch space affects the model complexity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n    def __init__(self, layer_size=192):\n        super(ConvNet, self).__init__()\n        self.layer_size = layer_size\n        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n        self.fc = nn.Linear(192, self.layer_size)\n        self.out = nn.Linear(self.layer_size, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n        x = x.view(-1, 192)\n        x = self.fc(x)\n        x = self.out(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef train(model, optimizer, train_loader, device=None):\n    device = device or torch.device(\"cpu\")\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n\n\ndef test(model, data_loader, device=None):\n    device = device or torch.device(\"cpu\")\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(data_loader):\n            data, target = data.to(device), target.to(device)\n            outputs = model(data)\n            _, predicted = torch.max(outputs.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n\n    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tune trainable for model selection\nWe'll now define our Tune trainable function. This function takes\na ``config`` parameter containing the hyperparameters we should train\nthe model on, and will start a full training run. This means it\nwill take care of creating the model and optimizer and repeatedly\ncall the ``train`` function to train the model. Also, this function\nwill report the training progress back to Tune.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_mnist(config,\n                start_model=None,\n                checkpoint_dir=None,\n                num_epochs=10,\n                use_gpus=False,\n                data_fn=None,\n                day=0):\n    # Create model\n    use_cuda = use_gpus and torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    model = ConvNet(layer_size=config[\"layer_size\"]).to(device)\n\n    # Create optimizer\n    optimizer = optim.SGD(\n        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n\n    # Load checkpoint, or load start model if no checkpoint has been\n    # passed and a start model is specified\n    load_dir = None\n    if checkpoint_dir:\n        load_dir = checkpoint_dir\n    elif start_model:\n        load_dir = start_model\n\n    if load_dir:\n        model_state, optimizer_state = torch.load(\n            os.path.join(load_dir, \"checkpoint\"))\n        model.load_state_dict(model_state)\n        optimizer.load_state_dict(optimizer_state)\n\n    # Get full training datasets\n    train_dataset, validation_dataset = data_fn(day=day)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n\n    validation_loader = torch.utils.data.DataLoader(\n        validation_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n\n    for i in range(num_epochs):\n        train(model, optimizer, train_loader, device)\n        acc = test(model, validation_loader, device)\n        if i == num_epochs - 1:\n            with tune.checkpoint_dir(step=i) as checkpoint_dir:\n                torch.save((model.state_dict(), optimizer.state_dict()),\n                           os.path.join(checkpoint_dir, \"checkpoint\"))\n            tune.report(mean_accuracy=acc, done=True)\n        else:\n            tune.report(mean_accuracy=acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuring the search space and starting Ray Tune\nWe would like to support two modes of training the model: Training\na model from scratch, and continuing to train a model from an\nexisting one.\n\nThis is our function to train a number of models with different\nhyperparameters from scratch, i.e. from all data that is available\nuntil the given day. Our search space can thus also contain parameters\nthat affect the model complexity (such as the layer size), since it\ndoes not have to be compatible to an existing model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def tune_from_scratch(num_samples=10, num_epochs=10, gpus_per_trial=0., day=0):\n    data_interface = MNISTDataInterface(\"/tmp/mnist_data\", max_days=10)\n    num_examples = data_interface._get_day_slice(day)\n\n    config = {\n        \"batch_size\": tune.choice([16, 32, 64]),\n        \"layer_size\": tune.choice([32, 64, 128, 192]),\n        \"lr\": tune.loguniform(1e-4, 1e-1),\n        \"momentum\": tune.uniform(0.1, 0.9),\n    }\n\n    scheduler = ASHAScheduler(\n        metric=\"mean_accuracy\",\n        mode=\"max\",\n        max_t=num_epochs,\n        grace_period=1,\n        reduction_factor=2)\n\n    reporter = CLIReporter(\n        parameter_columns=[\"layer_size\", \"lr\", \"momentum\", \"batch_size\"],\n        metric_columns=[\"mean_accuracy\", \"training_iteration\"])\n\n    analysis = tune.run(\n        partial(\n            train_mnist,\n            start_model=None,\n            data_fn=data_interface.get_data,\n            num_epochs=num_epochs,\n            use_gpus=True if gpus_per_trial > 0 else False,\n            day=day),\n        resources_per_trial={\n            \"cpu\": 1,\n            \"gpu\": gpus_per_trial\n        },\n        config=config,\n        num_samples=num_samples,\n        scheduler=scheduler,\n        progress_reporter=reporter,\n        verbose=0,\n        name=\"tune_serve_mnist_fromscratch\")\n\n    best_trial = analysis.get_best_trial(\"mean_accuracy\", \"max\", \"last\")\n    best_accuracy = best_trial.metric_analysis[\"mean_accuracy\"][\"last\"]\n    best_trial_config = best_trial.config\n    best_checkpoint = best_trial.checkpoint.value\n\n    return best_accuracy, best_trial_config, best_checkpoint, num_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To continue training from an existing model, we can use this function\ninstead. It takes a starting model (a checkpoint) as a parameter and\nthe old config.\n\nNote that this time the search space does _not_ contain the\nlayer size parameter. Since we continue to train an existing model,\nwe cannot change the layer size mid training, so we just continue\nto use the existing one.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def tune_from_existing(start_model,\n                       start_config,\n                       num_samples=10,\n                       num_epochs=10,\n                       gpus_per_trial=0.,\n                       day=0):\n    data_interface = MNISTDataInterface(\"/tmp/mnist_data\", max_days=10)\n    num_examples = data_interface._get_day_slice(day) - \\\n                   data_interface._get_day_slice(day - 1)\n\n    config = start_config.copy()\n    config.update({\n        \"batch_size\": tune.choice([16, 32, 64]),\n        \"lr\": tune.loguniform(1e-4, 1e-1),\n        \"momentum\": tune.uniform(0.1, 0.9),\n    })\n\n    scheduler = ASHAScheduler(\n        metric=\"mean_accuracy\",\n        mode=\"max\",\n        max_t=num_epochs,\n        grace_period=1,\n        reduction_factor=2)\n\n    reporter = CLIReporter(\n        parameter_columns=[\"lr\", \"momentum\", \"batch_size\"],\n        metric_columns=[\"mean_accuracy\", \"training_iteration\"])\n\n    analysis = tune.run(\n        partial(\n            train_mnist,\n            start_model=start_model,\n            data_fn=data_interface.get_incremental_data,\n            num_epochs=num_epochs,\n            use_gpus=True if gpus_per_trial > 0 else False,\n            day=day),\n        resources_per_trial={\n            \"cpu\": 1,\n            \"gpu\": gpus_per_trial\n        },\n        config=config,\n        num_samples=num_samples,\n        scheduler=scheduler,\n        progress_reporter=reporter,\n        verbose=0,\n        name=\"tune_serve_mnist_fromsexisting\")\n\n    best_trial = analysis.get_best_trial(\"mean_accuracy\", \"max\", \"last\")\n    best_accuracy = best_trial.metric_analysis[\"mean_accuracy\"][\"last\"]\n    best_trial_config = best_trial.config\n    best_checkpoint = best_trial.checkpoint.value\n\n    return best_accuracy, best_trial_config, best_checkpoint, num_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Serving tuned models with Ray Serve\nLet's now turn to the model serving part with Ray Serve. Serve\ndistinguishes between _backends_ and _endpoints_. Broadly speaking, a\nbackend handles incoming requests and replies with a result. For\ninstance, our MNIST backend takes an image as input and outputs the\ndigit it recognized from it. An endpoint on the other hand forwards\nincoming HTTP requests to one or more different backends, according\nto a routing policy.\n\nFirst, we will define our backend. This backend loads our PyTorch\nMNIST model from a checkpoint, takes an image as an input and\noutputs our digit prediction according to our trained model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MNISTBackend:\n    def __init__(self, checkpoint_dir, config, metrics, use_gpu=False):\n        self.checkpoint_dir = checkpoint_dir\n        self.config = config\n        self.metrics = metrics\n\n        use_cuda = use_gpu and torch.cuda.is_available()\n        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n        model = ConvNet(layer_size=self.config[\"layer_size\"]).to(self.device)\n\n        model_state, optimizer_state = torch.load(\n            os.path.join(self.checkpoint_dir, \"checkpoint\"),\n            map_location=self.device)\n        model.load_state_dict(model_state)\n\n        self.model = model\n\n    def __call__(self, flask_request):\n        images = torch.tensor(flask_request.json[\"images\"])\n        images = images.to(self.device)\n        outputs = self.model(images)\n        predicted = torch.max(outputs.data, 1)[1]\n        return {\"result\": predicted.numpy().tolist()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We would like to have a fixed location where we store the currently\nactive model. We call this directory ``model_dir``. Every time we\nwould like to update our model, we copy the checkpoint of the new\nmodel to this directory. We then create a new backend pointing to\nthat directory, route all the traffic on our model endpoint to this\nbackend, and then delete the old backends to free up some memory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def serve_new_model(model_dir, checkpoint, config, metrics, day, gpu=False):\n    print(\"Serving checkpoint: {}\".format(checkpoint))\n\n    checkpoint_path = _move_checkpoint_to_model_dir(model_dir, checkpoint,\n                                                    config, metrics)\n\n    try:\n        # Try to connect to an existing cluster.\n        client = serve.connect()\n    except RayServeException:\n        # If this is the first run, need to start the cluster.\n        client = serve.start(detached=True)\n\n    backend_name = \"mnist:day_{}\".format(day)\n\n    client.create_backend(backend_name, MNISTBackend, checkpoint_path, config,\n                          metrics, gpu)\n\n    if \"mnist\" not in client.list_endpoints():\n        # First time we serve a model - create endpoint\n        client.create_endpoint(\n            \"mnist\", backend=backend_name, route=\"/mnist\", methods=[\"POST\"])\n    else:\n        # The endpoint already exists, route all traffic to the new model\n        # Here you could also implement an incremental rollout, where only\n        # a part of the traffic is sent to the new backend and the\n        # rest is sent to the existing backends.\n        client.set_traffic(\"mnist\", {backend_name: 1.0})\n\n    # Delete previous existing backends\n    for existing_backend in client.list_backends():\n        if existing_backend.startswith(\"mnist:day\") and \\\n           existing_backend != backend_name:\n            client.delete_backend(existing_backend)\n\n    return True\n\n\ndef _move_checkpoint_to_model_dir(model_dir, checkpoint, config, metrics):\n    \"\"\"Move backend checkpoint to a central `model_dir` on the head node.\n    If you would like to run Serve on multiple nodes, you might want to\n    move the checkpoint to a shared storage, like Amazon S3, instead.\"\"\"\n    os.makedirs(model_dir, 0o755, exist_ok=True)\n\n    checkpoint_path = os.path.join(model_dir, \"checkpoint\")\n    meta_path = os.path.join(model_dir, \"meta.json\")\n\n    if os.path.exists(checkpoint_path):\n        shutil.rmtree(checkpoint_path)\n\n    shutil.copytree(checkpoint, checkpoint_path)\n\n    with open(meta_path, \"wt\") as fp:\n        json.dump(dict(config=config, metrics=metrics), fp)\n\n    return checkpoint_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we would like to continue training from the current existing\nmodel, we introduce an utility function that fetches the currently\nserved checkpoint as well as the hyperparameter config and achieved\naccuracy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_current_model(model_dir):\n    checkpoint_path = os.path.join(model_dir, \"checkpoint\")\n    meta_path = os.path.join(model_dir, \"meta.json\")\n\n    if not os.path.exists(checkpoint_path) or \\\n       not os.path.exists(meta_path):\n        return None, None, None\n\n    with open(meta_path, \"rt\") as fp:\n        meta = json.load(fp)\n\n    return checkpoint_path, meta[\"config\"], meta[\"metrics\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Putting everything together\nNow we only need to glue this code together. This is the main\nentrypoint of the script, and we will define three methods:\n\n1. Train new model from scratch with all data\n2. Continue training from existing model with new data only\n3. Query the model with test data\n\nInternally, this will just call the ``tune_from_scratch`` and\n``tune_from_existing()`` functions.\nBoth training functions will then call ``serve_new_model()`` to serve\nthe newly trained or updated model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The query function will send a HTTP request to Serve with some\n# test data obtained from the MNIST dataset.\nif __name__ == \"__main__\":\n    \"\"\"\n    This script offers training a new model from scratch with all\n    available data, or continuing to train an existing model\n    with newly available data.\n\n    For instance, we might get new data every day. Every Sunday, we\n    would like to train a new model from scratch.\n\n    Naturally, we would like to use hyperparameter optimization to\n    find the best model for out data.\n\n    First, we might train a model with all data available at this day:\n\n    .. code-block:: bash\n\n        python tune-serve-integration-mnist.py --from_scratch --day 0\n\n    On the coming days, we want to continue to train this model with\n    newly available data:\n\n    .. code-block:: bash\n\n        python tune-serve-integration-mnist.py --from_existing --day 1\n        python tune-serve-integration-mnist.py --from_existing --day 2\n        python tune-serve-integration-mnist.py --from_existing --day 3\n        python tune-serve-integration-mnist.py --from_existing --day 4\n        python tune-serve-integration-mnist.py --from_existing --day 5\n        python tune-serve-integration-mnist.py --from_existing --day 6\n        # Retrain from scratch every 7th day:\n        python tune-serve-integration-mnist.py --from_scratch --day 7\n\n    We can also use this script to query our served model\n    with some test data:\n\n    .. code-block:: bash\n\n        python tune-serve-integration-mnist.py --query 6\n        Querying model with example #6. Label = 1, Response = 1, Correct = T\n        python tune-serve-integration-mnist.py --query 28\n        Querying model with example #28. Label = 2, Response = 7, Correct = F\n\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"MNIST Tune/Serve example\")\n    parser.add_argument(\"--model_dir\", type=str, default=\"~/mnist_tune_serve\")\n\n    parser.add_argument(\n        \"--from_scratch\",\n        action=\"store_true\",\n        help=\"Train and select best model from scratch\",\n        default=False)\n\n    parser.add_argument(\n        \"--from_existing\",\n        action=\"store_true\",\n        help=\"Train and select best model from existing model\",\n        default=False)\n\n    parser.add_argument(\n        \"--day\",\n        help=\"Indicate the day to simulate the amount of data available to us\",\n        type=int,\n        default=0)\n\n    parser.add_argument(\n        \"--query\", help=\"Query endpoint with example\", type=int, default=-1)\n\n    parser.add_argument(\n        \"--smoke-test\",\n        action=\"store_true\",\n        help=\"Finish quickly for testing\",\n        default=False)\n\n    args = parser.parse_args()\n\n    if args.smoke_test:\n        ray.init(num_cpus=3)\n\n    model_dir = os.path.expanduser(args.model_dir)\n\n    if args.query >= 0:\n        import requests\n\n        dataset = MNISTDataInterface(\"/tmp/mnist_data\", max_days=0).dataset\n        data = dataset[args.query]\n        label = data[1]\n\n        # Query our model\n        response = requests.post(\n            \"http://localhost:8000/mnist\",\n            json={\"images\": [data[0].numpy().tolist()]})\n\n        try:\n            pred = response.json()[\"result\"][0]\n        except:  # noqa: E722\n            pred = -1\n\n        print(\"Querying model with example #{}. \"\n              \"Label = {}, Response = {}, Correct = {}\".format(\n                  args.query, label, pred, label == pred))\n        sys.exit(0)\n\n    gpus_per_trial = 0.5 if not args.smoke_test else 0.\n    serve_gpu = True if gpus_per_trial > 0 else False\n    num_samples = 8 if not args.smoke_test else 1\n    num_epochs = 10 if not args.smoke_test else 1\n\n    if args.from_scratch:  # train everyday from scratch\n        print(\"Start training job from scratch on day {}.\".format(args.day))\n        acc, config, best_checkpoint, num_examples = tune_from_scratch(\n            num_samples, num_epochs, gpus_per_trial, day=args.day)\n        print(\"Trained day {} from scratch on {} samples. \"\n              \"Best accuracy: {:.4f}. Best config: {}\".format(\n                  args.day, num_examples, acc, config))\n        serve_new_model(model_dir, best_checkpoint, config, acc, args.day,\n                        serve_gpu)\n\n    if args.from_existing:\n        old_checkpoint, old_config, old_acc = get_current_model(model_dir)\n        if not old_checkpoint or not old_config or not old_acc:\n            print(\"No existing model found. Train one with --from_scratch \"\n                  \"first.\")\n            sys.exit(1)\n        acc, config, best_checkpoint, num_examples = tune_from_existing(\n            old_checkpoint,\n            old_config,\n            num_samples,\n            num_epochs,\n            gpus_per_trial,\n            day=args.day)\n        print(\"Trained day {} from existing on {} samples. \"\n              \"Best accuracy: {:.4f}. Best config: {}\".format(\n                  args.day, num_examples, acc, config))\n        serve_new_model(model_dir, best_checkpoint, config, acc, args.day,\n                        serve_gpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's it! We now have an end-to-end workflow to train and update a\nmodel every day with newly arrived data. Every week we might retrain\nthe whole model. At every point in time we make sure to serve the\nmodel that achieved the best validation set accuracy.\n\nThere are some ways we might extend this example. For instance, right\nnow we only serve the latest trained model. We could  also choose to\nroute only a certain percentage of users to the new model, maybe to\nsee if the new model really does it's job right. These kind of\ndeployments are called `canary deployments <serve-split-traffic>`.\nThese kind of deployments would also require us to keep more than one\nmodel in our ``model_dir`` - which should be quite easy: We could just\ncreate subdirectories for each training day.\n\nStill, this example should show you how easy it is to integrate the\nRay libraries Ray Tune and Ray Serve in your workflow. While both tools\nalso work independently of each other, they complement each other\nnicely and support a large number of use cases.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}